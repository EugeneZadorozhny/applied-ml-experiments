{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20b8739",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671b4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\zador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be55c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "Stop_words = stopwords.words(\"english\")\n",
    "Sentences = gutenberg.sents(\"carroll-alice.txt\")\n",
    "TermsSentences = []\n",
    "for terms in Sentences:\n",
    "    terms = [w for w in terms if w not in Stop_words]\n",
    "    terms = [w for w in terms if re.search(r\"^[a-zA-Z]{2}\", w) is not None]\n",
    "    TermsSentences.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8871654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bea4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filename = \"results/Alice-sentences.csv\"\n",
    "\n",
    "with open(Filename, \"w\") as fout:\n",
    "    writer = csv.writer(fout, delimiter=\",\", lineterminator=\"\\n\")\n",
    "    for i in range(len(TermsSentences)):\n",
    "        writer.writerow(TermsSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89794b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=2793 items, N=1703 transactions\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "Transactions_list = []  # a list of transactions\n",
    "Items_names = {}  # Lookup item ID to name\n",
    "Items_ids = {}  # Lookup item name to ID\n",
    "\n",
    "Items = None  # a list of item IDs, normally an increasing sequence of numbers\n",
    "\n",
    "# Process the data\n",
    "with open(\"results/Alice-sentences.csv\", \"r\") as fin:\n",
    "    reader = csv.reader(fin, delimiter=\",\")\n",
    "    item_id = 0\n",
    "    for row in reader:\n",
    "        transaction = []\n",
    "        for item in row:\n",
    "            if item not in Items_ids:\n",
    "                Items_ids[item] = item_id\n",
    "                Items_names[item_id] = item\n",
    "                item_id += 1\n",
    "            transaction += [Items_ids[item]]\n",
    "        Transactions_list += [transaction]\n",
    "\n",
    "M, N = len(Items_ids), len(Transactions_list)\n",
    "\n",
    "Items = np.arange(0, M)\n",
    "\n",
    "# Information, sanity\n",
    "print(f\"M={M} items, N={N} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c333295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'Adventures', 'Wonderland', 'Lewis', 'Carroll', 'CHAPTER', 'Down']\n",
      "[[0, 1, 2, 3, 4], [5], [6, 7, 8], [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 13, 19, 20, 21, 22, 18, 23, 0, 24, 20, 25], [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 7, 50, 51, 52, 53], [54, 15, 55, 56, 0, 57, 55, 58, 59, 60, 7, 61, 62, 63], [62, 63]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print([Items_names[_] for _ in Items[0:7]])\n",
    "print(Transactions_list[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea6f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "Transactions = np.full((N, M), False, dtype=bool)\n",
    "\n",
    "for i, t in enumerate(Transactions_list):\n",
    "    for item in t:\n",
    "        Transactions[i][item] = True\n",
    "\n",
    "# Sanity, print row index 10, 11\n",
    "print(f\"{Transactions[10:12].astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c468bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filename = \"EP_datasets/input_for_weka.csv\"\n",
    "\n",
    "with open(Filename, \"w\") as fout:\n",
    "    writer = csv.writer(\n",
    "        fout, delimiter=\",\", quoting=csv.QUOTE_ALL, quotechar=\"'\", lineterminator=\"\\n\"\n",
    "    )\n",
    "    writer.writerow([Items_names[i] for i in range(M)])\n",
    "    for i in range(N):\n",
    "        writer.writerow(\n",
    "            list(map(lambda x: \"\" if x == False else \"True\", Transactions[i]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bf8c8bd",
   "metadata": {},
   "source": [
    "=== Run information ===\n",
    "\n",
    "Scheme:       weka.associations.FPGrowth -P 2 -I -1 -N 100 -T 0 -C 0.9 -D 0.05 -U 1.0 -M 0.004\n",
    "Relation:     input_for_weka\n",
    "Instances:    1703\n",
    "Attributes:   2793\n",
    "              [list of attributes omitted]\n",
    "=== Associator model (full training set) ===\n",
    "\n",
    "FPGrowth found 20 rules (displaying top 20)\n",
    "\n",
    " 1. [golden=True]: 7 ==> [little=True]: 7   <conf:(1)> lift:(15.07) lev:(0) conv:(6.54) \n",
    " 2. [Mock=True]: 56 ==> [Turtle=True]: 56   <conf:(1)> lift:(29.36) lev:(0.03) conv:(54.09) \n",
    " 3. [White=True]: 22 ==> [Rabbit=True]: 22   <conf:(1)> lift:(39.6) lev:(0.01) conv:(21.44) \n",
    " 4. [Hare=True]: 30 ==> [March=True]: 30   <conf:(1)> lift:(54.94) lev:(0.02) conv:(29.45) \n",
    " 5. [gardeners=True]: 7 ==> [three=True]: 7   <conf:(1)> lift:(63.07) lev:(0) conv:(6.89) \n",
    " 6. [join=True]: 9 ==> [dance=True]: 9   <conf:(1)> lift:(131) lev:(0.01) conv:(8.93) \n",
    " 7. [oop=True]: 7 ==> [Soo=True]: 7   <conf:(1)> lift:(243.29) lev:(0) conv:(6.97) \n",
    " 8. [Soo=True]: 7 ==> [oop=True]: 7   <conf:(1)> lift:(243.29) lev:(0) conv:(6.97) \n",
    " 9. [said=True, Turtle=True]: 32 ==> [Mock=True]: 32   <conf:(1)> lift:(30.41) lev:(0.02) conv:(30.95) \n",
    "10. [said=True, Mock=True]: 32 ==> [Turtle=True]: 32   <conf:(1)> lift:(29.36) lev:(0.02) conv:(30.91) \n",
    "11. [said=True, White=True]: 9 ==> [Rabbit=True]: 9   <conf:(1)> lift:(39.6) lev:(0.01) conv:(8.77) \n",
    "12. [said=True, Hare=True]: 15 ==> [March=True]: 15   <conf:(1)> lift:(54.94) lev:(0.01) conv:(14.73) \n",
    "13. [table=True, glass=True]: 7 ==> [little=True]: 7   <conf:(1)> lift:(15.07) lev:(0) conv:(6.54) \n",
    "14. [Turtle=True, Gryphon=True]: 8 ==> [Mock=True]: 8   <conf:(1)> lift:(30.41) lev:(0) conv:(7.74) \n",
    "15. [Mock=True, Gryphon=True]: 8 ==> [Turtle=True]: 8   <conf:(1)> lift:(29.36) lev:(0) conv:(7.73) \n",
    "16. [March=True]: 31 ==> [Hare=True]: 30   <conf:(0.97)> lift:(54.94) lev:(0.02) conv:(15.23) \n",
    "17. [Turtle=True]: 58 ==> [Mock=True]: 56   <conf:(0.97)> lift:(29.36) lev:(0.03) conv:(18.7) \n",
    "18. [said=True, March=True]: 16 ==> [Hare=True]: 15   <conf:(0.94)> lift:(53.22) lev:(0.01) conv:(7.86) \n",
    "19. [Of=True]: 11 ==> [course=True]: 10   <conf:(0.91)> lift:(64.51) lev:(0.01) conv:(5.42) \n",
    "20. [glass=True]: 10 ==> [little=True]: 9   <conf:(0.9)> lift:(13.56) lev:(0) conv:(4.67) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c84d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden1=30,\n",
    "        n_hidden2=30,\n",
    "        epochs=100,\n",
    "        eta=0.001,\n",
    "        minibatch_size=1,\n",
    "        seed=None,\n",
    "    ):\n",
    "        self.random = np.random.RandomState(seed)  # used to randomize weights\n",
    "        self.n_hidden1 = n_hidden1  # size of the hidden layer 1\n",
    "        self.n_hidden2 = n_hidden2  # size of the hidden layer 2\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = (\n",
    "            minibatch_size  # size of training batch - 1 would not work\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def onehot(y, n_classes):  # one hot encode the input class y\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):  # Eq 1\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def _forward(self, X):  # Eq 2\n",
    "        # hidden layer 1\n",
    "        z_h1 = np.dot(X, self.w_h1)\n",
    "        a_h1 = self.sigmoid(z_h1)\n",
    "\n",
    "        # hidden layer 2\n",
    "        z_h2 = np.dot(a_h1, self.w_h2)\n",
    "        a_h2 = self.sigmoid(z_h2)\n",
    "\n",
    "        # output\n",
    "        z_out = np.dot(a_h2, self.w_out)\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        return z_h1, a_h1, z_h2, a_h2, z_out, a_out\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Eq 4\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        import sys\n",
    "\n",
    "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "        self.w_out = self.random.normal(\n",
    "            loc=0.0, scale=0.1, size=(self.n_hidden2, n_output)\n",
    "        )\n",
    "\n",
    "        # 2 hidden layers\n",
    "        self.w_h2 = self.random.normal(\n",
    "            loc=0.0, scale=0.1, size=(self.n_hidden1, self.n_hidden2)\n",
    "        )\n",
    "        self.w_h1 = self.random.normal(\n",
    "            loc=0.0, scale=0.1, size=(n_features, self.n_hidden1)\n",
    "        )\n",
    "\n",
    "        y_train_enc = self.onehot(y_train, n_output)  # one-hot encode original y\n",
    "        for i in range(self.epochs):  # Ideally must shuffle at every epoch\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            for start_idx in range(\n",
    "                0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size\n",
    "            ):\n",
    "                batch_idx = indices[start_idx : start_idx + self.minibatch_size]\n",
    "\n",
    "                z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(\n",
    "                    X_train[batch_idx]\n",
    "                )  # neural network model\n",
    "\n",
    "                sigmoid_derivative_h1 = a_h1 * (1.0 - a_h1)  # Eq 3\n",
    "                sigmoid_derivative_h2 = a_h2 * (1.0 - a_h2)  # Eq 3\n",
    "\n",
    "                delta_out = a_out - y_train_enc[batch_idx]  # Eq 5\n",
    "\n",
    "                delta_h2 = (\n",
    "                    np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h2\n",
    "                )  # Eq 6\n",
    "                delta_h1 = np.dot(delta_h2, self.w_h2.T) * sigmoid_derivative_h1  # Eq 6\n",
    "\n",
    "                grad_w_out = np.dot(a_h2.T, delta_out)  # Eq 7\n",
    "                grad_w_h2 = np.dot(a_h1.T, delta_h2)  # Eq 7\n",
    "                grad_w_h1 = np.dot(X_train[batch_idx].T, delta_h1)  # Eq 8\n",
    "\n",
    "                self.w_out -= self.eta * grad_w_out  # Eq 9\n",
    "                self.w_h2 -= self.eta * grad_w_h2  # Eq 9\n",
    "                self.w_h1 -= self.eta * grad_w_h1  # Eq 9\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(\n",
    "                X_train\n",
    "            )  # monitoring training progress through reclassification\n",
    "            y_valid_pred = self.predict(\n",
    "                X_valid\n",
    "            )  # monitoring training progress through validation\n",
    "            train_acc = (np.sum(y_train == y_train_pred)).astype(float) / X_train.shape[\n",
    "                0\n",
    "            ]\n",
    "            valid_acc = (np.sum(y_valid == y_valid_pred)).astype(float) / X_valid.shape[\n",
    "                0\n",
    "            ]\n",
    "            sys.stderr.write(\n",
    "                \"\\r%d/%d | Cost: %.2f \"\n",
    "                \"| Train/Valid Acc.: %.2f%%/%.2f%% \"\n",
    "                % (i + 1, self.epochs, cost, train_acc * 100, valid_acc * 100)\n",
    "            )\n",
    "            sys.stderr.flush()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dabb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import ranf\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def load_mnist(path, kind=\"train\"):\n",
    "    from numpy import fromfile, uint8\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    labels_path = os.path.join(path, \"%s-labels-idx1-ubyte\" % kind)\n",
    "    images_path = os.path.join(path, \"%s-images-idx3-ubyte\" % kind)\n",
    "    with open(labels_path, \"rb\") as lbpath:\n",
    "        magic, n = struct.unpack(\">II\", lbpath.read(8))\n",
    "        labels = fromfile(lbpath, dtype=uint8)\n",
    "        with open(images_path, \"rb\") as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "            images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "            images = ((images / 255.0) - 0.5) * 2\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_train, y_train = load_mnist(\"./EP_datasets/mnist/\", kind=\"train\")\n",
    "print(f\"Rows= {X_train.shape[0]}, columns= {X_train.shape[1]}\")\n",
    "\n",
    "X_test, y_test = load_mnist(\"./EP_datasets/mnist/\", kind=\"t10k\")\n",
    "print(f\"Rows= {X_test.shape[0]}, columns= {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08bc429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50/50 | Cost: 17517.23 | Train/Valid Acc.: 95.43%/95.58% "
     ]
    }
   ],
   "source": [
    "# Define and fit the neural network\n",
    "nn = NeuralNetMLP(\n",
    "    n_hidden1=20, n_hidden2=20, epochs=50, eta=0.0005, minibatch_size=100, seed=1\n",
    ")\n",
    "\n",
    "nn.fit(\n",
    "    X_train=X_train[:55000],\n",
    "    y_train=y_train[:55000],\n",
    "    X_valid=X_train[55000:],\n",
    "    y_valid=y_train[55000:],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab42582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def get_acc(_y_test, _y_pred):\n",
    "    return (np.sum(_y_test == _y_pred)).astype(float) / _y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073398fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 94.51%\n",
      "[[ 962    0    1    1    1    6    5    1    3    0]\n",
      " [   0 1109    3    2    0    2    5    4   10    0]\n",
      " [  18    2  958    9    7    3    9   10   16    0]\n",
      " [   1    1   20  931    1   24    1    9   17    5]\n",
      " [   1    0    5    0  937    1    6    3   10   19]\n",
      " [  10    1    4   20    1  820    8    1   22    5]\n",
      " [  10    2    3    0    7   16  912    0    8    0]\n",
      " [   3    9   15   12    6    2    0  963    3   15]\n",
      " [   3    2    0   12    4   15    4    4  927    3]\n",
      " [   6    4    0   10   23    8    1   10   15  932]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy= {get_acc(y_test,y_pred)*100:.2f}%\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a00d2",
   "metadata": {},
   "source": [
    "The performance of the 2 layer network for this problem is equally as good as the 1 layer network with only 50 epochs compared to the 300 from the 1 layer example. This means that the 2 layer network converges to an answer faster and therefore can be trained faster than the 1 layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39836592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
